# -*- coding: utf-8 -*-
"""Decision Tree Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/123Cu3AiEXBXwwC2Yi_1G8lUqeCWUl3AO
"""

import math
from collections import Counter
from typing import List, Dict, Union, Any

def calculate_entropy(labels: List[str]) -> float:
    label_counts = Counter(labels)
    entropy = 0.0
    for count in label_counts.values():
        p = count / len(labels)
        entropy -= p * math.log2(p)
    return round(entropy, 4)

def calculate_information_gain(examples: List[Dict[str, Any]], attribute: str, target_attr: str) -> float:
    # Compute total entropy
    labels = [ex[target_attr] for ex in examples]
    total_entropy = calculate_entropy(labels)

    # Partition by attribute values
    values = {ex[attribute] for ex in examples}
    weighted_entropy = 0.0
    for value in values:
        subset_labels = [ex[target_attr] for ex in examples if ex[attribute] == value]
        prob = len(subset_labels) / len(examples)
        weighted_entropy += prob * calculate_entropy(subset_labels)

    information_gain = total_entropy - weighted_entropy
    return information_gain

def find_best_attribute(examples: List[Dict[str, Any]], attributes: List[str], target_attr: str) -> str:
    best_gain = -1.0
    best_attribute = attributes[0] if attributes else None
    for attribute in attributes:
        gain = calculate_information_gain(examples, attribute, target_attr)
        if gain > best_gain:
            best_gain = gain
            best_attribute = attribute
    return best_attribute

def majority_class(examples: List[Dict[str, Any]], target_attr: str) -> str:
    labels = [ex[target_attr] for ex in examples]
    counts = Counter(labels)
    return counts.most_common(1)[0][0]

def learn_decision_tree(
    examples: List[Dict[str, Any]],
    attributes: List[str],
    target_attr: str
) -> Union[dict, str]:
    # If all examples have the same label, return that label (leaf)
    first_label = examples[0][target_attr]
    if all(ex[target_attr] == first_label for ex in examples):
        return first_label

    # If no attributes left, return the majority class (leaf)
    if not attributes:
        return majority_class(examples, target_attr)

    # Choose the best attribute and split
    best_attribute = find_best_attribute(examples, attributes, target_attr)
    tree: Dict[str, Any] = {best_attribute: {}}

    values = {ex[best_attribute] for ex in examples}
    for value in values:
        subset = [ex for ex in examples if ex[best_attribute] == value]
        if not subset:
            tree[best_attribute][value] = majority_class(examples, target_attr)
        else:
            remaining_attributes = [a for a in attributes if a != best_attribute]
            tree[best_attribute][value] = learn_decision_tree(subset, remaining_attributes, target_attr)

    return tree