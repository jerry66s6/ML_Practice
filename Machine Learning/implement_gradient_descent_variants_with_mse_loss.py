# -*- coding: utf-8 -*-
"""Implement Gradient Descent Variants with MSE Loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/123Cu3AiEXBXwwC2Yi_1G8lUqeCWUl3AO
"""

import numpy as np

def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):
	m = len(y)
	for iterations in range(n_iterations):
		if method == 'batch':
			predictions = np.dot(X, weights)
			errors = predictions - y
			part = np.dot(X.T, errors)
			weights = weights-(learning_rate*2/m)*part
		elif method == 'stochastic':
			for i in range(m):
				xi = X[i:i+1]
				yi = y[i:i+1]
				prediction = np.dot(xi, weights)
				error = prediction -yi
				part = np.dot(xi.T, error)
				weights = weights-(learning_rate*2)*part
		else:
			for i in range(0, m, batch_size):
				end = i + batch_size
				xb = X[i:end]
				yb = y[i:end]
				predictions = np.dot(xb, weights)
				error = predictions -yb
				part = np.dot(xb.T, error)
				weights = weights-(learning_rate*2/batch_size)*part
	return weights