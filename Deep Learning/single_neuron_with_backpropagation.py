# -*- coding: utf-8 -*-
"""Single Neuron with Backpropagation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/123Cu3AiEXBXwwC2Yi_1G8lUqeCWUl3AO
"""

import numpy as np

def train_neuron(
    features: np.ndarray,
    labels: np.ndarray,
    initial_weights: np.ndarray,
    initial_bias: float,
    learning_rate: float,
    epochs: int
) -> (np.ndarray, float, list[float]):
    # Track MSE over epochs
    mse_values = []

    for epoch in range(epochs):
        mse = 0.0
        cache_w = np.zeros_like(initial_weights)
        cache_b = 0.0

        for i in range(len(features)):
            z = np.dot(features[i], initial_weights) + initial_bias
            g = 1 / (1 + np.exp(-z))  # sigmoid
            sigma = g * (1 - g)

            # Accumulate MSE
            mse += (g - labels[i]) ** 2

            # Gradients accumulate (before averaging and factor 2)
            cache_w += (g - labels[i]) * sigma * features[i]
            cache_b += (g - labels[i]) * sigma

        # Mean over samples
        mse /= len(labels)
        mse_values.append(mse)

        # Scale gradients for MSE: d/dÎ¸ of (g - y)^2 gives 2*(g - y)*...
        devw = (2 / len(labels)) * cache_w
        devb = (2 / len(labels)) * cache_b

        # Gradient step
        initial_weights -= learning_rate * devw
        initial_bias -= learning_rate * devb

    updated